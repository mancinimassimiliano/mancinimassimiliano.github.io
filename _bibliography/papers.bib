---
---


@inproceedings{chaudhuri2023transitivity,
  bibtex_show={true},
  title={Transitivity Recovering Decompositions: Interpretable and Robust Fine-Grained Relationships},
  author={Chaudhuri, Abhra and Mancini, Massimiliano and Akata, Zeynep and Dutta, Anjan},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  preview={trd.png},
  html={https://openreview.net/forum?id=wUNPmdE273}, 
  pdf={https://openreview.net/pdf?id=wUNPmdE273},
  code={https://github.com/abhrac/trd},   
  selected={false},
  abstract={Recent advances in fine-grained representation learning leverage local-to-global (emergent) relationships for achieving state-of-the-art results. The relational representations relied upon by such methods, however, are abstract. We aim to deconstruct this abstraction by expressing them as interpretable graphs over image views. We begin by theoretically showing that abstract relational representations are nothing but a way of recovering transitive relationships among local views. Based on this, we design Transitivity Recovering Decompositions (TRD), a graph-space search algorithm that identifies interpretable equivalents of abstract emergent relationships at both instance and class levels, and with no post-hoc computations. We additionally show that TRD is provably robust to noisy views, with empirical evidence also supporting this finding. The latter allows TRD to perform at par or even better than the state-of-the-art, while being fully interpretable.}
}




@inproceedings{vasconcelos2019structured,
  bibtex_show={true},
  title={Structured Domain Adaptation for 3D Keypoint Estimation},
  author={Vasconcelos, Levi O and Mancini, Massimiliano and Boscaini, Davide and Caputo, Barbara and Ricci, Elisa},
  booktitle={2019 International Conference on 3D Vision (3DV)},
  year={2019},
  organization={IEEE},
  preview={3dv2019.png},
  html={https://ieeexplore.ieee.org/abstract/document/8885979},
  selected={false},
  abstract={Motivated by recent advances in deep domain adaptation, this paper introduces a deep architecture for estimating 3D keypoints when the training (source) and the test (target) images greatly differ in terms of visual appearance (domain shift). Our approach operates by promoting domain distribution alignment in the feature space adopting batch normalization-based techniques. Furthermore, we propose to collect statistics about 3D keypoints positions of the source training data and to use this prior information to constrain predictions on the target domain introducing a loss derived from Multidimensional Scaling. We conduct an extensive experimental evaluation considering three publicly available benchmarks and show that our approach out-performs state-of-the-art domain adaptation methods for 3D keypoints predictions.}
}


@inproceedings{Cugu_2022_CVPR,
  bibtex_show={true},
    author    = {Cugu, Ilke and Mancini, Massimiliano and Chen, Yanbei and Akata, Zeynep},
    title     = {Attention Consistency on Visual Corruptions for Single-Source Domain Generalization},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2022},
  preview={acvc2022},
  html={https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Cugu_Attention_Consistency_on_Visual_Corruptions_for_Single-Source_Domain_Generalization_CVPRW_2022_paper.html},
  pdf={https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/papers/Cugu_Attention_Consistency_on_Visual_Corruptions_for_Single-Source_Domain_Generalization_CVPRW_2022_paper.pdf},
  code={https://github.com/ExplainableML/ACVC},
  selected={false},
  abstract={Generalizing visual recognition models trained on a single distribution to unseen input distributions (i.e. domains) requires making them robust to superfluous correlations in the training set. In this work, we achieve this goal by altering the training images to simulate new domains and imposing consistent visual attention across the different views of the same sample. We discover that the first objective can be simply and effectively met through visual corruptions. Specifically, we alter the content of the training images using the nineteen corruptions of the ImageNet-C benchmark and three additional transformations based on Fourier transform. Since these corruptions preserve object locations, we propose an attention consistency loss to ensure that class activation maps across original and corrupted versions of the same training sample are aligned. We name our model Attention Consistency on Visual Corruptions (ACVC). We show that ACVC consistently achieves the state of the art on three single-source domain generalization benchmarks, PACS, COCO, and the large-scale DomainNet.}
}


@inproceedings{cermelli2021prototype,
  bibtex_show={true},
  title={Prototype-based Incremental Few-Shot Semantic Segmentation},
  author={Cermelli, Fabio and Mancini, Massimiliano and Xian, Yongqin and Akata, Zeynep and Caputo, Barbara},
  booktitle={British Machine Vision Conference},
  year={2021},
  preview={bmvc2021.png},
  pdf={https://www.bmvc2021-virtualconference.com/assets/papers/0484.pdf},
  code={https://github.com/fcdl94/FSS},
  selected={false},
  abstract={Semantic segmentation models have two fundamental weaknesses: i) they require large training sets with costly pixel-level annotations, and ii) they have a static output space, constrained to the classes of the training set. Toward addressing both problems, we introduce a new task, Incremental Few-Shot Segmentation (iFSS). The goal of iFSS is to extend a pretrained segmentation model with new classes from few annotated images and without access to old training data. To overcome the limitations of existing models iniFSS, we propose Prototype-based Incremental Few-Shot Segmentation (PIFS) that couples prototype learning and knowledge distillation. PIFS exploits prototypes to initialize the classifiers of new classes, fine-tuning the network to refine its features representation. We design a prototype-based distillation loss on the scores of both old and new class prototypes to avoid overfitting and forgetting, and batch-renormalization to cope with non-i.i.d.few-shot data. We create an extensive benchmark for iFSS showing that PIFS outperforms several few-shot and incremental learning methods in all scenarios.}
}


@inproceedings{chaudhuri2022cross,
  bibtex_show={true},
  title={Cross-Modal Fusion Distillation for Fine-Grained Sketch-Based Image Retrieval},
  author={Chaudhuri, Abhra and Mancini, Massimiliano and Chen, Yanbei and Akata, Zeynep and Dutta, Anjan},
  booktitle={British Machine Vision Conference},
  year={2022},
  preview={bmvc2022.png},
  pdf={https://bmvc2022.mpi-inf.mpg.de/0499.pdf},
  code={https://github.com/abhrac/xmodal-vit},
  selected={false},
  abstract={Representation learning for sketch-based image retrieval has mostly been tackled by learning embeddings that discard modality-specific information. As instances from different modalities can often provide complementary information describing the underlying concept, we propose a cross-attention framework for Vision Transformers (XModalViT) that fuses modality-specific information instead of discarding them. Our framework first maps paired datapoints from the individual photo and sketch modalities to fused representations that unify information from both modalities. We then decouple the input space of the aforementioned modality fusion network into independent encoders of the individual modalities via contrastive and relational cross-modal knowledge distillation. Such encoders can then be applied to downstream tasks like cross-modal retrieval. We demonstrate the expressive capacity of the learned representations by performing a wide range of experiments and achieving state-of-the-art results on three fine-grained sketch-based image retrieval benchmarks: Shoe-V2, Chair-V2 and Sketchy. }
}


@article{conti2023vocabulary,
  bibtex_show={true},
  title={Vocabulary-free Image Classification},
  author={Conti, Alessandro and Fini, Enrico and Mancini, Massimiliano and Rota, Paolo and Wang, Yiming and Ricci, Elisa},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2023},
  preview={vic.png},
  html={https://alessandroconti.me/papers/2306.00917},  
  pdf={https://arxiv.org/pdf/2306.00917.pdf},
  code={https://github.com/altndrr/vic},   
  selected={true},
  abstract={Recent advances in large vision-language models have revolutionized the image classification paradigm. Despite showing impressive zero-shot capabilities, a pre-defined set of categories, a.k.a. the vocabulary, is assumed at test time for composing the textual prompts. However, such assumption can be impractical when the semantic context is unknown and evolving. We thus formalize a novel task, termed as Vocabulary-free Image Classification (VIC), where we aim to assign to an input image a class that resides in an unconstrained language-induced semantic space, without the prerequisite of a known vocabulary. VIC is a challenging task as the semantic space is extremely large, containing millions of concepts, with hard-to-discriminate fine-grained categories.}
}


@article{9745371,
  bibtex_show={true},
  author={Mancini, Massimiliano and Naeem, Muhammad Ferjad and Xian, Yongqin and Akata, Zeynep},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Learning Graph Embeddings for Open World Compositional Zero-Shot Learning}, 
  year={2024},
  volume={46},
  number={3},
  pages={1545-1560},
  doi={10.1109/TPAMI.2022.3163667},
  selected={false},
  preview={co-cge-pami2022.png},
  code={https://github.com/ExplainableML/co-cge},
  pdf={https://arxiv.org/pdf/2105.01017.pdf},
  html={https://ieeexplore.ieee.org/abstract/document/9745371/},
  abstract={Compositional Zero-Shot learning (CZSL) aims to recognize unseen compositions of state and object visual primitives seen during training. A problem with standard CZSL is the assumption of knowing which unseen compositions will be available at test time. In this work, we overcome this assumption operating on the open world setting, where no limit is imposed on the compositional space at test time, and the search space contains a large number of unseen compositions. To address this problem, we propose a new approach, Compositional Cosine Graph Embeddings (Co-CGE), based on two principles. First, Co-CGE models the dependency between states, objects and their compositions through a graph convolutional neural network. The graph propagates information from seen to unseen concepts, improving their representations. Second, since not all unseen compositions are equally feasible, and less feasible ones may damage the learned representations, Co-CGE estimates a feasibility score for each unseen composition, using the scores as margins in a cosine similarity-based loss and as weights in the adjacency matrix of the graphs. Experiments show that our approach achieves state-of-the-art performances in standard CZSL while outperforming previous methods in the open world scenario.}
 }

@inproceedings{dutta2021concurrent,
  bibtex_show={true},
  title={Concurrent Discrimination and Alignment for Self-Supervised Feature Learning},
  author={Dutta, Anjan and Mancini, Massimiliano and Akata, Zeynep},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},
  pages={2189--2198},
  year={2021},
  preview={codial.png},
  html={https://arxiv.org/abs/2108.08562},
  pdf={https://arxiv.org/pdf/2108.08562.pdf},
  code={https://github.com/AnjanDutta/codial},
  selected={false},
  abstract={Existing self-supervised learning methods learn representation by means of pretext tasks which are either (1) discriminating that explicitly specify which features should be separated or (2) aligning that precisely indicate which features should be closed together, but ignore the fact how to jointly and principally define which features to be repelled and which ones to be attracted. In this work, we combine the positive aspects of the discriminating and aligning methods, and design a hybrid method that addresses the above issue. Our method explicitly specifies the repulsion and attraction mechanism respectively by discriminative predictive task and concurrently maximizing mutual information between paired views sharing redundant information. We qualitatively and quantitatively show that our proposed model learns better features that are more effective for the diverse downstream tasks ranging from classification to semantic segmentation. Our experiments on nine established benchmarks show that the proposed model consistently outperforms the existing state-of-the-art results of self-supervised and transfer learning protocol.}
}


@inproceedings{mancini2018boosting,
  bibtex_show={true},
	author = {Mancini, Massimilano and Porzi, Lorenzo and Rota Bulò, Samuel and Caputo, Barbara and Ricci, Elisa},
  	title  = {Boosting Domain Adaptation by Discovering Latent Domains},
  	booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  	year      = {2018},
  preview={latent-domains.png},
  html={https://openaccess.thecvf.com/content_cvpr_2018/html/Mancini_Boosting_Domain_Adaptation_CVPR_2018_paper.html},
  pdf={https://openaccess.thecvf.com/content_cvpr_2018/papers/Mancini_Boosting_Domain_Adaptation_CVPR_2018_paper.pdf},
  code={https://github.com/mancinimassimiliano/latent_domains_DA},
  selected={false},
  abstract={Current Domain Adaptation (DA) methods based on deep architectures assume that the source samples arise from a single distribution. However, in practice most datasets can be regarded as mixtures of multiple domains. In these cases exploiting single-source DA methods for learning target classifiers may lead to sub-optimal, if not poor, results. In addition, in many applications it is difficult to manually provide the domain labels for all source data points, ie latent domains should be automatically discovered. This paper introduces a novel Convolutional Neural Network (CNN) architecture which (i) automatically discovers latent domains in visual datasets and (ii) exploits this information to learn robust target classifiers. Our approach is based on the introduction of two main components, which can be embedded into any existing CNN architecture:(i) a side branch that automatically computes the assignment of a source sample to a latent domain and (ii) novel layers that exploit domain membership information to appropriately align the distribution of the CNN internal feature representations to a reference distribution. We test our approach on publicly-available datasets, showing that it outperforms state-of-the-art multi-source DA methods by a large margin.},
}


@inproceedings{Cermelli_2020_CVPR,
  bibtex_show={true},
author = {Cermelli, Fabio and Mancini, Massimiliano and Bulò, Samuel Rota and Ricci, Elisa and Caputo, Barbara},
title = {Modeling the Background for Incremental Learning in Semantic Segmentation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2020},
  preview={latent-domains.png},
  html={https://openaccess.thecvf.com/content_CVPR_2020/html/Cermelli_Modeling_the_Background_for_Incremental_Learning_in_Semantic_Segmentation_CVPR_2020_paper.html},
  pdf={https://openaccess.thecvf.com/content_CVPR_2020/papers/Cermelli_Modeling_the_Background_for_Incremental_Learning_in_Semantic_Segmentation_CVPR_2020_paper.pdf},
  code={https://github.com/fcdl94/MiB},
  selected={true},
  abstract={Despite their effectiveness in a wide range of tasks, deep architectures suffer from some important limitations. In particular, they are vulnerable to catastrophic forgetting, ie they perform poorly when they are required to update their model as new classes are available but the original training set is not retained. This paper addresses this problem in the context of semantic segmentation. Current strategies fail on this task because they do not consider a peculiar aspect of semantic segmentation: since each training step provides annotation only for a subset of all possible classes, pixels of the background class (ie pixels that do not belong to any other classes) exhibit a semantic distribution shift. In this work we revisit classical incremental learning methods, proposing a new distillation-based framework which explicitly accounts for this shift. Furthermore, we introduce a novel strategy to initialize classifier's parameters, thus preventing biased predictions toward the background class. We demonstrate the effectiveness of our approach with an extensive evaluation on the Pascal-VOC 2012 and ADE20K datasets, significantly outperforming state of the art incremental learning methods.}
}


@inproceedings{mancini2021open,
  bibtex_show={true},
  title={Open World Compositional Zero-Shot Learning},
  author={Mancini, Massimiliano and Naeem, Muhammad Ferjad and Xian, Yongqin and Akata, Zeynep},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  preview={cvpr2021.png},
  html={https://openaccess.thecvf.com/content/CVPR2021/html/Mancini_Open_World_Compositional_Zero-Shot_Learning_CVPR_2021_paper.html},  
  pdf={https://openaccess.thecvf.com/content/CVPR2021/papers/Mancini_Open_World_Compositional_Zero-Shot_Learning_CVPR_2021_paper.pdf},
  code={https://github.com/ExplainableML/czsl},   
  selected={true},
  abstract={Compositional Zero-Shot learning (CZSL) requires to recognize state-object compositions unseen during training. In this work, instead of assuming prior knowledge about the unseen compositions, we operate in the open world setting, where the search space includes a large number of unseen compositions some of which might be unfeasible. In this setting, we start from the cosine similarity between visual features and compositional embeddings. After estimating the feasibility score of each composition, we use these scores to either directly mask the output space or as a margin for the cosine similarity between visual features and compositional embeddings during training. Our experiments on two standard CZSL benchmarks show that all the methods suffer severe performance degradation when applied in the open world setting. While our simple CZSL model achieves state-of-the-art performances in the closed world scenario, our feasibility scores boost the performance of our approach in the open world setting, clearly outperforming the previous state of the art.}
}


@inproceedings{Karthik_2022_CVPR,
  bibtex_show={true},
    author    = {Karthik, Shyamgopal and Mancini, Massimiliano and Akata, Zeynep},
    title     = {KG-SP: Knowledge Guided Simple Primitives for Open World Compositional Zero-Shot Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2022},
  preview={cvpr2022.png},
  html={http://openaccess.thecvf.com/content/CVPR2022/html/Karthik_KG-SP_Knowledge_Guided_Simple_Primitives_for_Open_World_Compositional_Zero-Shot_CVPR_2022_paper.html},
  pdf={http://openaccess.thecvf.com/content/CVPR2022/papers/Karthik_KG-SP_Knowledge_Guided_Simple_Primitives_for_Open_World_Compositional_Zero-Shot_CVPR_2022_paper.pdf},
  code={https://github.com/ExplainableML/KG-SP},
  selected={false},
  abstract={The goal of open-world compositional zero-shot learning (OW-CZSL) is to recognize compositions of state and objects in images, given only a subset of them during training and no prior on the unseen compositions. In this setting, models operate on a huge output space, containing all possible state-object compositions. While previous works tackle the problem by learning embeddings for the compositions jointly, here we revisit a simple CZSL baseline and predict the primitives, ie states and objects, independently. To ensure that the model develops primitive-specific features, we equip the state and object classifiers with separate, non-linear feature extractors. Moreover, we estimate the feasibility of each composition through external knowledge, using this prior to remove unfeasible compositions from the output space. Finally, we propose a new setting, ie CZSL under partial supervision (pCZSL), where either only objects or state labels are available during training and we can use our prior to estimate the missing labels. Our model, Knowledge-Guided Simple Primitives (KG-SP), achieves the state of the art in both OW-CZSL and pCZSL, surpassing most recent competitors even when coupled with semi-supervised learning techniques}
}


@inproceedings{caldarola2021cluster,
  bibtex_show={true},
author = {Caldarola, Debora and Mancini, Massimiliano and Galasso, Fabio and Ciccone, Marco and Rodolà, Emanuele and Caputo, Barbara},
title = {Cluster-driven Graph Federated Learning over Multiple Domains},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
year = {2021},
  preview={federated-2021.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{fontanel2021detecting,
  bibtex_show={true},
author={Fontanel, Dario and Cermelli, Fabio and Mancini, Massimiliano and Caputo, Barbara},
title = {Detecting Anomalies in Semantic Segmentation with Prototypes},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
year = {2021},
  preview={anomaly2021.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{pastore2021closer,
  bibtex_show={true},
author = {Pastore, Giuseppe and Cermelli, Fabio and Xian, Yongqin and Mancini, Massimiliano and Akata, Zeynep and Caputo, Barbara},
title = {A Closer Look at Self-training for Zero-Label Semantic Segmentation},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2021},
  preview={zsl2021.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{mancini2020towards,
  bibtex_show={true},
author={Mancini, Massimiliano and Akata, Zeynep and Ricci, Elisa and Caputo, Barbara},
title={Towards Recognizing Unseen Categories in Unseen Domains},
booktitle={Proceedings of the European Conference on Computer Vision (ECCV) 2020},
year={2020},
  preview={eccv2020.png},
  html={},  
  pdf={},
  code={},   
  selected={true},
  abstract={}
}

@inproceedings{alaniz2022abstracting,
  bibtex_show={true},
  title={Abstracting Sketches through Simple Primitives},
  author={Alaniz, Stephan and Mancini, Massimiliano and Dutta, Anjan and Marcos, Diego and Akata, Zeynep},
  booktitle="Proceedings of the European Conference on Computer Vision (ECCV) 2022",
  year={2022},
  preview={PMN_teaser.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{upadhyay2022bayescap,
  bibtex_show={true},
  title={BayesCap: Bayesian Identity Cap for Calibrated Uncertainty in Frozen Neural Networks},
  author={Upadhyay, Uddeshya and Karthik, Shyamgopal and Chen, Yanbei and Mancini, Massimiliano and Akata, Zeynep},
  booktitle="Proceedings of the European Conference on Computer Vision (ECCV) 2022",
  year={2022},
  preview={BayesCap.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{christensen2023image,
  bibtex_show={true},
  title={Image-free Classifier Injection for Zero-Shot Classification},
  author={Christensen, Anders and Mancini, Massimiliano and Koepke, A. Sophia and Winther, Ole and Akata, Zeynep},
  booktitle="Proceedings of the International Conference on Computer Vision (ICCV) 2023",
  year={2023},
  preview={ICIS_teaser.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{alaniz2023iterative,
  bibtex_show={true},
  title={Iterative Superquadric Recomposition of 3D Objects from Multiple Views},
  author={Alaniz, Stephan and Mancini, Massimiliano and Akata, Zeynep},
  booktitle="Proceedings of the International Conference on Computer Vision (ICCV) 2023",
  year={2023},
  preview={isco_teaser.png},
  html={},  
  pdf={},
  code={https://github.com/ExplainableML/ISCO},   
  selected={false},
  abstract={}
}


@inproceedings{vanderklis2023pdisconet,
  bibtex_show={true},
  title={PDiscoNet: Semantically consistent part discovery for fine-grained recognition},
  author={van der Klis, Robert and Alaniz, Stephan and Mancini, Massimiliano and Dantas, Cassio F. and Ienco, Dino and Akata, Zeynep and Marcos, Diego},
  booktitle="Proceedings of the International Conference on Computer Vision (ICCV) 2023",
  year={2023},
  preview={PDISCO_teaser.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}

@inproceedings{upadhyay2023probvlm,
  bibtex_show={true},
  title={ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models},
  author={Upadhyay, Uddeshya and Karthik, Shyamgopal and Mancini, Massimiliano and Akata, Zeynep},
  booktitle="Proceedings of the International Conference on Computer Vision (ICCV) 2023",
  year={2023},
  preview={ProbVLM_teaser.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}

@inproceedings{demin2023effectiveness,
  bibtex_show={true},
  title={On the Effectiveness of LayerNorm Tuning for Continual Learning in Vision Transformers},
  author={De Min, Thomas and Mancini, Massimiliano and Alahari, Karteek and Alameda-Pineda, Xavier and Ricci, Elisa},
  booktitle="The First Workshop on Visual Continual Learning at ICCV 2023",
  year={2023},
  preview={PDISCO_teaser.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@article{mancini2019discovering,
  bibtex_show={true},
title={Discovering Latent Domains for Unsupervised Domain Adaptation Through Consistency},
author={Mancini, Massimiliano and Porzi, Lorenzo and Cermelli, Fabio and Caputo, Barbara},
booktitle = {International Conference on Image Analysis and Processing (ICIAP)},
year = {2019},
month = {September},
  preview={iciap2019.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{mancini2018best,
  bibtex_show={true},
	author = {Mancini, Massimilano and Rota Bulò, Samuel and Caputo, Barbara and Ricci, Elisa},
  	title  = {Best sources forward: domain generalization through source-specific nets},
  	booktitle = {IEEE International Conference on Image Processing (ICIP)},
  	year      = {2018},
  	month     = {October},
  preview={icip2018.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{vasconcelos2020shape,
  bibtex_show={true},
  author={Vasconcelos, Levi O. and Mancini, Massimiliano and Boscaini, Davide and Bulò, Samuel Rota and Caputo, Barbara and Ricci, Elisa},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Shape Consistent 2D Keypoint Estimation under Domain Shift}, 
  year={2021},
  pages={8037-8044},
  preview={icpr2020.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{mancini2019knowledge,
  bibtex_show={true},
	author = {Mancini, Massimilano and Karaoguz, Hakan and Ricci, Elisa and Jensfelt, Patric and Caputo, Barbara},
  	title  = {Knowledge is Never Enough: Towards Web Aided Deep Open World Recognition},
  	booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},
  	year      = {2019},
  	month     = {May},
  preview={icra2019.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{mancini2018kitting,
  bibtex_show={true},
	author = {Mancini, Massimilano and Karaoguz, Hakan and Ricci, Elisa and Jensfelt, Patric and Caputo, Barbara},
  	title  = {Kitting in the Wild through Online Domain Adaptation},
  	booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  	year      = {2018},
  	month     = {October},
  preview={iros2018.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@article{mancini2020boosting,
  bibtex_show={true},
  title={Boosting binary masks for multi-domain learning through affine transformations},
  author={Mancini, Massimiliano and Ricci, Elisa and Caputo, Barbara and Bulò, Samuel Rota},
  journal={Machine Vision and Applications},
  volume={31},
  number={6},
  pages={1--14},
  year={2020},
  publisher={Springer},
  preview={mva2020.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@article{chaudhuri2022relational,
  bibtex_show={true},
  title={Relational Proxies: Emergent Relationships as Fine-Grained Discriminators},
  author={Chaudhuri, Abhra and Mancini, Massimiliano and Akata, Zeynep and Dutta, Anjan},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2022},
  preview={neurips2022.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@article{mancini2021inferring,
  bibtex_show={true},
  author={Mancini, Massimiliano and Porzi, Lorenzo and Buló, Samuel Rota and Caputo, Barbara and Ricci, Elisa},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Inferring Latent Domains for Unsupervised Deep Domain Adaptation}, 
  year={2021},
  volume={43},
  number={2},
  pages={485-498},
  doi={10.1109/TPAMI.2019.2933829},
  preview={pami2019.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@article{cermelli2021modeling,
  bibtex_show={true},
  title={Modeling the Background for Incremental and Weakly-Supervised Semantic Segmentation},
  author={Cermelli, Fabio and Mancini, Massimiliano and Bulò, Samuel Rota and Ricci, Elisa and Caputo, Barbara},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  publisher={IEEE},
  preview={pami2021.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@article{mancini2018learning,
  bibtex_show={true},
	author={Massimiliano Mancini and Samuel Rota Bulò and Elisa Ricci and Barbara Caputo},
	journal={IEEE Robotics and Automation Letters},
	title={Learning Deep NBNN Representations for Robust Place Categorization},
	year={2017},
	volume={2},
	number={3},
	pages={1794-1801},
	doi={10.1109/LRA.2017.2705282},
	month={July},
  preview={learning-deep-nbnn.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@article{mancini2018robust,
  bibtex_show={true},
	author={Massimiliano Mancini and Samuel Rota Bulò and Barbara Caputo and Elisa Ricci},
	journal={IEEE Robotics and Automation Letters},
	title={Robust Place Categorization With Deep Domain Generalization},
	year={2018},
	volume={3},
	number={3},
	pages={2093-2100},
	doi={10.1109/LRA.2018.2809700},
	month={July},
  preview={deep-domain-generalization.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@article{fontanel2020boosting,
  bibtex_show={true},
  author={Fontanel, Dario and Cermelli, Fabio and Mancini, Massimiliano and Bulò, Samuel Rota and Ricci, Elisa and Caputo, Barbara},
  journal={IEEE Robotics and Automation Letters}, 
  title={Boosting Deep Open World Recognition by Clustering}, 
  year={2020},
  volume={5},
  number={4},
  pages={5985-5992},
  preview={ral2020.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@article{fontanel2021challenges,
  bibtex_show={true},
  author={Fontanel, Dario and Cermelli, Fabio and Mancini, Massimiliano and Caputo, Barbara},
  journal={IEEE Robotics and Automation Letters}, 
  title={On the Challenges of Open World Recognition Under Shifting Visual Domains}, 
  year={2021},
  volume={6},
  number={2},
  pages={604-611},
  preview={ral2021.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@article{chen2022semi,
  bibtex_show={true},
  title={Semi-Supervised and Unsupervised Deep Visual Learning: A Survey},
  author={Chen, Yanbei and Mancini, Massimiliano and Zhu, Xiatian and Akata, Zeynep},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE},
  preview={survey2022.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{mancini2017embedding,
  bibtex_show={true},
  title={Embedding Words and Senses Together via Joint Knowledge-Enhanced Training},
  author={Mancini, Massimiliano and Camacho-Collados, Jose and Iacobacci, Ignacio and Navigli, Roberto},
  booktitle={Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)},
  pages={100--111},
  year={2017},
  preview={sw2v-img.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{mancini2018adding,
  bibtex_show={true},
	author = {Mancini, Massimilano and Ricci, Elisa and Caputo, Barbara and Rota Bulò, Samuel},
  	title  = {Adding New Tasks to a Single Network with Weight Transformations using Binary Masks},
  	booktitle = {Proceedings of the European Conference on Computer Vision (ECCV) Workshops},
  	year      = {2018},
  	month     = {September},
  preview={taskcv2018.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@phdthesis{mancini2020phd,
  bibtex_show={true},
    title    = {Towards Recognizing New Semantic Concepts in New Visual Domains},
    school   = {Sapienza University of Rome},
    author   = {Mancini, Massimiliano},
    year     = {2020},
  preview={thesis.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{karthik2021revisiting,
  bibtex_show={true},
  title={Revisiting Visual Product for Compositional Zero-Shot Learning},
  author={Karthik, Shyamgopal and Mancini, Massimiliano and Akata, Zeynep},
  booktitle={NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications},
  year={2021},
  preview={visprod2021.png},
  html={},  
  pdf={},
  code={},   
  selected={false},
  abstract={}
}


@inproceedings{mancini2019adagraph,
  bibtex_show={true},
  title={Adagraph: Unifying predictive and continuous domain adaptation through graphs},
  author={Mancini, Massimiliano and Bulò, Samuel Rota and Caputo, Barbara and Ricci, Elisa},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6568--6577},
  year={2019},
  preview={cvpr2019.png},
  html={},  
  pdf={},
  code={},   
  selected={true},
  abstract={}
}

